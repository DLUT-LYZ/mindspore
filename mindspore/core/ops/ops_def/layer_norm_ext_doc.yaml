layer_norm_ext:
  description: |  
    Layer normalization (Lei Ba and et al., 2016).
      Applies layer normalization to the n-dimensional input array.
      This operator takes an n-dimensional input array and normalizes
    the input using the given axis:
  
      .. math::
      
      out = \frac{data - mean(data, axis)}{\sqrt{var(data, axis)+\epsilon}}
      * gamma + beta
  
      Unlike batch normalization, the mean and var are computed along the channel dimension.
  
      Assume the input has size k on axis 1, then both gamma and beta have shape (k,).
  
      .. note::
      
      This operator can be optimized away for inference.
  
      Parameters
      ----------
    input : tvm.relay.Expr
      Input to which layer_norm will be applied.
    
    normalized_shape :
    
    weight : 
    
    bias : 
    
    eps : double, optional, default=1e-5
      Small float added to variance to avoid dividing by zero.
    
      Returns
      -------
    output : tvm.relay.Expr
      The normalized data.
    mean_out : 
    rstd_out :

